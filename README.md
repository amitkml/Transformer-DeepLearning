# Transformer-DeepLearning
EVA Transformerbased Deep Learning course. This repo contains all my nlp work and learning. Made public so that others can learn and get benefits.The repo will contain all my project related to NLP learning and vision model deployment using mediapipe.

## Pytorch Package Hierarchy
![Pytorch](https://manalelaidouni.github.io/assets/img/pexels/Pytorch-package-hierarchy.jpg)

## Network Visualization
- [Tools-to-Design-or-Visualize-Architecture-of-Neural-Network](https://github.com/ashishpatel26/Tools-to-Design-or-Visualize-Architecture-of-Neural-Network)

## Different types of Convolution

### Dilated convolution
- dilated convolutions are used to increase the receptive field of the higher layers, compensating for the reduction in receptive field induced by removing subsampling.

![im](https://www.researchgate.net/publication/336002670/figure/fig1/AS:806667134455815@1569335840531/An-illustration-of-the-receptive-field-for-one-dilated-convolution-with-different.png)
![im](https://miro.medium.com/max/875/1*btockft7dtKyzwXqfq70_w.gif)

A scenario of dilated convolution for kernel size 3Ã—3. From the top: (a) it is the situation of the standard convolutional layer when the dilation rate is (1,1). (b) when the dilation rate become (2,2) the receptive field increases. (c) in the last case, the dilation rate is (3,3) and the receptive field enlarges even more than situation b.

[im](https://www.researchgate.net/profile/Mohammad-Hamed-Mozaffari/publication/335390357/figure/fig2/AS:795761700794376@1566735782667/A-scenario-of-dilated-convolution-for-kernel-size-33-From-the-top-a-it-is-the.jpg)
